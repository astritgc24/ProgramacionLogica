#!/usr/bin/env python
# coding: utf-8

# # Caso Práctico: Random Forest
# 
# Eneste caso práctico se pretende resolver un problema de deteccion de Malware en dispositivos android mediante el análisis del trafico de red que genera el dispositivo mediante el uso de subconjuntos de arboles de decisión. 
# 
# ### DataSet: Detección de Malware en Android
# 
# #### Descripcion 
# The sophisticated and advanced Android malware is able to identify the presence of the emulator used by the malware analyst and in response, alter its behaviour to evade detection. To overcome this issue, we installed the Android applications on the real device and captured its network traffic. 
# 
# CICAAGM dataset is captured by installing the Android apps on the real smartphones semi-automated. The dataset is generated from 1,900 applications with the following three categories:
# 
# 1. Adware (250 apps)
# 
# Airpush: Designed to deliver unsolicited advertisements to the user’s systems for information stealing.
# 
# Dowgin: Designed as an advertisement library that can also steal the user’s information.
# 
# Kemoge: Designed to take over a user’s Android device. This adware is a hybrid of botnet and disguises itself as popular apps via repackaging.
# 
# Mobidash: Designed to display ads and to compromise user’s personal information.
# 
# Shuanet: Similar to Kemoge, Shuanet is also designed to take over a user’s device.
# 
# 2. General Malware (150 apps)
# 
# AVpass: Designed to be distributed in the guise of a Clock app.
# 
# FakeAV: Designed as a scam that tricks user to purchase a full version of the software in order to re-mediate non-existing infections.
# 
# FakeFlash/FakePlayer: Designed as a fake Flash app in order to direct users to a website (after successfully installed).
# 
# GGtracker: Designed for SMS fraud (sends SMS messages to a premium-rate number) and information stealing.
# 
# Penetho: Designed as a fake service (hacktool for Android devices that can be used to crack the WiFi password). The malware is also able to infect the user’s computer via infected email attachment, fake updates, external media and infected documents.
# 
# 3. Benign (1,500 apps)
# 
# 2015 GooglePlay market (top free popular and top free new)
# 
# 2016 GooglePlay market (top free popular and top free new)
# 
# License
# 
# The CICAAGM dataset consists of the following items is publicly available for researchers.
# 
# .pcap files – the network traffic of both the malware and benign (20% malware and 80% benign)
# 
# .csv files - the list of extracted network traffic features generated by the CIC-flowmeter
# 
# If you are using our dataset, you should cite our related paper that outlines the details of the dataset and its underlying principles:
# 
# Arash Habibi Lashkari, Andi Fitriah A. Kadir, Hugo Gonzalez, Kenneth Fon Mbah and Ali A. Ghorbani, “Towards a Network-Based Framework for Android Malware Detection and Characterization”, In the proceeding of the 15th International Conference on Privacy, Security and Trust, PST, Calgary, Canada, 2017.

# ## Imports

# In[4]:


import pandas as pd
import numpy as np 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import f1_score


# ## Funciones Auxiliares

# In[6]:


# Construcción de una función que realize el particionado completo.
def train_val_test_split(df, rstate=42, shuffle=True, stratify=None):
    strat = df[stratify] if stratify else None 
    train_set, test_set = train_test_split(
        df, test_size = 0.4, random_state = rstate, shuffle = shuffle, stratify = strat)
    strat = train_test[stratify] if stratify else None
    val_set, test_set = train_test_split(
        test_set, test_size=0.5, random_state = rstate, shuffle=shuffle, stratify = strat)
    return (train_set, val_set, test_set)


# In[7]:


def remove_labels(df, label_name):
    X = df.drop(label_name, axis=1)
    y = df[label_name].copy()
    return (X, y)


# In[8]:


def evaluate_result(y_pred, y, y_prep_pred, y_prep, metric):
    print(metric.__name__,"WITHOUT preparation:", metric(y_pred, y, average="weighted"))
    print(metric.__name__,"WITH preparation:", metric(y_prep_pred, y_prep, average="weighted"))


# ## 1.- Lectura del DataSet

# In[10]:


df = pd.read_csv("datasets/AndroidAdware2017/TotalFeatures-ISCXFlowMeter.csv")


# ## 2.- Visualizacion del DataSet

# In[12]:


df.head(10)


# In[13]:


df.describe()


# In[14]:


df.info()


# In[15]:


print("Longitud del DataSet", len(df))
print("Numero de características del DataSet", len(df.columns))


# In[16]:


df["calss"].value_counts()


# #### Buscando correlaciones

# In[18]:


# Transformamos la variable de salida a numerica para calcular correlciones
X = df.copy()
X['calss'] = X['calss'].factorize()[0]


# In[19]:


# Calcular las correlaciones
corr_matrix = X.corr()
corr_matrix["calss"].sort_values(ascending = False)


# In[20]:


X.corr()


# In[21]:


# Se puede llegar a valorar y quedarnos con aquellas que tengan mayor correlacion
corr_matrix[corr_matrix['calss'] > 0.05]


# ## 3.- Division del DataSet

# In[23]:


# Dividir el DataSet 
train_set, val_set, test_set = train_val_test_split(df)


# In[24]:


X_train, y_train = remove_labels(train_set, 'calss')
X_val, y_val = remove_labels(val_set, 'calss')
X_test, y_test = remove_labels(test_set, 'calss')


# ## 4.- Escalado del DataSet
# 
# Es importante comprender que los arboles de decision son algoritmos que **No requieren demasiada preparacion de los datos** correctamente no requieren la realizacion o escalado o normalizacion. En este ejercicio se va a realizar escalado al dataset y se van a comparar los resultados con el conjunto de datos sin escalar de esta manera se demuestra como aplicar preprocesamientos como el escalado puede llegar a afectar el rendimento del modelo 

# In[26]:


scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)


# In[27]:


scaler = RobustScaler()
X_test_scaled = scaler.fit_transform(X_test)


# In[28]:


scaler = RobustScaler()
X_val_scaled = scaler.fit_transform(X_val)


# In[29]:


# IMPORTAR DATAFRAME DE PANDAS
from pandas import DataFrame


# In[30]:


# Realizar la transformación a un DataFrame de pandas
X_train_scaled = DataFrame(X_train_scaled, columns = X_train.columns, index=X_train.index)
X_train_scaled.head(10)


# In[31]:


X_train_scaled.describe()


# ## 5.- Decision Forest

# In[33]:


# Modelo entrenado con el DataSet sin escalar
from sklearn.tree import DecisionTreeClassifier

clf_tree = DecisionTreeClassifier(random_state = 42)
clf_tree.fit(X_train, y_train)


# In[34]:


# Predecir con el DataSet de entrenamiento
y_train_pred = clf_tree.predict(X_train)


# In[35]:


print("F1 Score Train Set:", f1_score(y_train_pred, y_train, average="weighted"))


# In[36]:


# Predecir con el DataSet de validación

y_val_pred = clf_tree.predict(X_val)


# In[37]:


#Comparar resultados entre el escalado sin escalar
print("F1 Score Validation Set:", f1_score(y_val_pred, y_val, average="weighted"))


# ## 6.- Random Forest

# In[39]:


from sklearn.ensemble import RandomForestClassifier

# Módelo entrenado con el DataSet sin escalar

clf_rnd = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = -1)
clf_rnd.fit(X_train, y_train)


# In[40]:


# Modelo de entrenamiento con el DataSet escalado
clf_rnd_scaled = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = -1)
clf_rnd_scaled.fit(X_train_scaled, y_train)


# In[41]:


# Predecir con el DataSet de entrenamiento
y_train_pred = clf_rnd.predict(X_train)
y_train_prep_pred = clf_rnd_scaled.predict(X_train_scaled)


# In[42]:


# Comparar resultados entre el escalado y sin escalar
evaluate_result(y_train_pred, y_train, y_train_prep_pred, y_train, f1_score)


# ## 7.- Regresion Forest

# In[44]:


from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder


# In[45]:


label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)  # Convierte etiquetas a valores numéricos


# In[46]:


clf_rndr = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
clf_rndr.fit(X_train, y_train_encoded)


# In[47]:


# Modelo de entrenamiento con el DataSet escalado
clf_rndr_scaled = RandomForestRegressor(n_estimators = 100, random_state = 42, n_jobs = -1)
clf_rndr_scaled.fit(X_train_scaled, y_train_encoded)


# In[48]:


# Predecir con el DataSet de entrenamiento
y_train_pred = clf_rndr.predict(X_train)
y_train_prep_pred = clf_rndr_scaled.predict(X_train_scaled)


# In[54]:


from sklearn.metrics import mean_squared_error

# Definición de la función evaluate_result
def evaluate_result(y_pred, y, y_prep_pred, y_prep, metric):
    # Calcular y mostrar el MSE para el modelo sin escalado
    mse_without_scaling = metric(y_pred, y)
    print(f"{metric.__name__} WITHOUT preparation: {mse_without_scaling:.4f}")
    
    # Calcular y mostrar el MSE para el modelo con escalado
    mse_with_scaling = metric(y_prep_pred, y_prep)
    print(f"{metric.__name__} WITH preparation: {mse_with_scaling:.4f}")

# Comparar resultados entre el escalado y sin escalar
evaluate_result(y_train_pred, y_train_encoded, y_train_prep_pred, y_train_encoded, mean_squared_error)


# In[52]:


import matplotlib.pyplot as plt

# Realizar las predicciones
y_train_pred = clf_rndr.predict(X_train)
y_train_prep_pred = clf_rndr_scaled.predict(X_train_scaled)

# Crear la gráfica
plt.figure(figsize=(10, 5))

# Gráfico de dispersión para datos sin escalar
plt.subplot(1, 2, 1)
plt.scatter(y_train_encoded, y_train_pred, alpha=0.5, color='blue', label='Sin Escalar')
plt.plot([y_train_encoded.min(), y_train_encoded.max()], [y_train_encoded.min(), y_train_encoded.max()], 'k--', lw=2)
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.title('Sin Escalar')
plt.legend()

# Gráfico de dispersión para datos escalados
plt.subplot(1, 2, 2)
plt.scatter(y_train_encoded, y_train_prep_pred, alpha=0.5, color='green', label='Con Escalado')
plt.plot([y_train_encoded.min(), y_train_encoded.max()], [y_train_encoded.min(), y_train_encoded.max()], 'k--', lw=2)
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.title('Con Escalado')
plt.legend()

plt.tight_layout()
plt.show()


# In[ ]:




